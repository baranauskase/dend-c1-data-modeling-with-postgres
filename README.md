# Introduction

In this project, I'll apply what I've learned on data modeling with Postgres 
 course (part of Udacity Data Engineering Nanodegree) and build an ETL pipeline using Python. To complete the project, 
I will need to define fact and dimension tables for a star schema for a particular 
analytic focus, and write an ETL pipeline that transfers data from files in two local 
directories into these tables in Postgres using Python and SQL.

# Background

A startup called Sparkify wants to analyze the data they've been collecting on 
songs and user activity on their new music streaming app. The analytics team is 
particularly interested in understanding what songs users are listening to. 
Currently, they don't have an easy way to query their data, which resides in a 
directory of JSON logs on user activity on the app, as well as a directory with 
JSON metadata on the songs in their app.

The purpose of this project is to create a Postgres database with tables designed 
to optimize queries on song play analysis. The tasks include creating a database schema and ETL pipeline for this analysis. Testing of database and ETL pipeline can be performed by running queries provided by analytics team from Sparkify and comparing results with their expected results.



# Datasets

## Songs
The first dataset is a subset of real data from the [Million Song](http://millionsongdataset.com/) Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

## Logs

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

# Development environment
This project can be implemented on a local machine using Python3 and Docker. The 
instructions here assume that VS Code is used as a development environment,
especially the way `ipython` kernel is setup instead of using `jupyter notebook`.
It is good practice not to contaminate your global Python installation by
installing dependencies that are specific to this project. The first step 
therefore is to create a virtual Python environment. We will utilise the most
basic approach by creating an environment using `venv` module. At the root of 
the project run the following commands.

```shell
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
ipython kernel install --user --name=dend-c1-data-modeling-with-postgres
```

Now that the virtual environment is ready. The next step is to spin up a `PostgresSQL`
server. The cleanest way is to run an official `Docker` container and we can do
so by deploying the stack.

```shell
docker stack deploy -c postgres-stack.yml postgres
```
If the `Docker` engine is not running in `swarm` mode then it is also possible to
use `docker-compose` to deploy this stack.

# Project structure

1. `test.ipynb` displays the first few rows of each table to let us check the database.
2. `create_tables.py` drops and creates the tables. This file can be run to reset tables.
3. `etl.ipynb` reads and processes a single file from song_data and log_data and loads the data into the tables. The purpose of this notebook is to analyse the data.
4. `etl.py` reads and processes all files from song_data and log_data and loads them into the tables.
5. `sql_queries.py` contains all sql queries.

# Next steps
- Insert data using the COPY command to bulk insert log files instead of using INSERT on one row at a time
- Add data quality checks
- Create a dashboard for analytic queries on the new database